{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Alignment Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "import re\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import razdel\n",
    "\n",
    "from scipy import spatial\n",
    "from matplotlib import pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29c4c9071ae492a8a1c041cfaa267d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=642.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a31b0f380004737a767580f34575b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1649718.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7bc904d4f5e4029a7730ce34f74b442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=112.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87cd8ba1ec5f40c5a948a5373a312ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=24.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922e2f47de954ded8a3629b64dbebef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=711456784.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "pattern_ru_orig = re.compile(r'[a-zA-Z\\(\\)\\[\\]\\/\\<\\>•\\'\\n]+')\n",
    "double_spaces = re.compile(r'[\\s]+')\n",
    "double_commas = re.compile(r'[,]+')\n",
    "double_dash = re.compile(r'[-—]+')\n",
    "pattern_zh = re.compile(r'[」「“”„‟\\x1a⓪①②③④⑤⑥⑦⑧⑨⑩⑴⑵⑶⑷⑸⑹⑺⑻⑼⑽*a-zA-Zа-яА-Я\\(\\)\\[\\]\\s\\n\\/\\-\\:•＂＃＄％＆＇（）＊＋－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》【】〔〕〖〗〘〙〜〟〰〾〿–—‘’‛‧﹏〉]+')\n",
    "pat_comma = re.compile(r'[\\.]+')\n",
    "first_numbers = re.compile(r'^[0-9,\\.]+')\n",
    "last_punct = re.compile(r'[,\\.]+$')\n",
    "multiple_spaces = re.compile(r'\\s+')\n",
    "pattern_ru = re.compile(r'[a-zA-Z\\.\\(\\)\\[\\]\\/\\-\\:!?\\<\\>;•\\\"\\'«»——,]+')\n",
    "pattern_ru_letters_only = re.compile(r'[^а-яА-Я\\s]+')\n",
    "\n",
    "def split_zh(paragraph):\n",
    "    for sent in re.findall(u'[^!?。！？\\.\\!\\?]+[!?。！？\\.\\!\\?]?', paragraph, flags=re.U):\n",
    "        yield sent\n",
    "            \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased-sentence\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"DeepPavlov/rubert-base-cased-sentence\")\n",
    "\n",
    "def get_sent_vec(text):\n",
    "    tokenized_text = tokenizer.encode(text, add_special_tokens=True)\n",
    "    #indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor(tokenized_text).unsqueeze(0)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        return model(tokens_tensor)[0]\n",
    "\n",
    "#лемматизация\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def get_file_vectors(path_ru):\n",
    "    ru_vectors = []\n",
    "    with open(path_ru, mode=\"r\", encoding=\"utf-8\") as input_ru:\n",
    "        for line_ru in input_ru.readlines():\n",
    "            line_ru = re.sub(pattern_ru, '', line_ru).strip()\n",
    "            line_ru = re.sub(last_punct, '', line_ru).strip()\n",
    "            line_ru = re.sub(first_numbers, '', line_ru).strip()\n",
    "            line_ru = re.sub(multiple_spaces, ' ', line_ru).strip()\n",
    "            line_ru = line_ru.lower()\n",
    "\n",
    "            line_ru = ' '.join(list(map(lambda x: morph.parse(x)[0].normal_form, line_ru.split())))\n",
    "            #print(line_ru)\n",
    "            ru_vectors.append(get_sent_vec(line_ru))\n",
    "    return ru_vectors\n",
    "\n",
    "def get_line_vectors(lines):\n",
    "    ru_vectors = []\n",
    "    for line_ru in lines:\n",
    "        line_ru = re.sub(pattern_ru, '', line_ru).strip()\n",
    "        line_ru = re.sub(last_punct, '', line_ru).strip()\n",
    "        line_ru = re.sub(first_numbers, '', line_ru).strip()\n",
    "        line_ru = re.sub(multiple_spaces, ' ', line_ru).strip()\n",
    "        line_ru = line_ru.lower()\n",
    "\n",
    "        line_ru = ' '.join(list(map(lambda x: morph.parse(x)[0].normal_form, line_ru.split())))\n",
    "        #print(line_ru)\n",
    "        ru_vectors.append(get_sent_vec(line_ru))\n",
    "    return ru_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(ru_lines, zh_lines, ru_proxy_lines, sim_matrix, threshold):\n",
    "    ru = []\n",
    "    zh = []\n",
    "    ru_proxy = []\n",
    "    sims = []\n",
    "    for i in range(sim_matrix.shape[0]):\n",
    "        for j in range(sim_matrix.shape[1]):\n",
    "            if sim_matrix[i,j] >= threshold:\n",
    "                ru.append(ru_lines[j])\n",
    "                zh.append(zh_lines[i])\n",
    "                ru_proxy.append(ru_proxy_lines[i])\n",
    "                sims.append(sim_matrix[i,j])\n",
    "                \n",
    "    return ru,zh,ru_proxy,sims\n",
    "\n",
    "def get_sim_matrix(ru_vec, ru_vec2, window=10):\n",
    "    sim_matrix=np.zeros((len(ru_vec), len(ru_vec2)))\n",
    "    k = len(ru_vec)/len(ru_vec2)\n",
    "    for i in range(len(ru_vec)):\n",
    "        for j in range(len(ru_vec2)):\n",
    "            if (j*k > i-window) & (j*k < i+window):\n",
    "                sim = 1 - spatial.distance.cosine(ru_vec[i][0,-1,:], ru_vec2[j][0,-1,:])\n",
    "                sim_matrix[i,j] = sim\n",
    "\n",
    "    return sim_matrix  \n",
    "\n",
    "def align_texts(base_path, file_ru, file_zh, threshold, batch_size=50, window=10, n_gram=1, n_gram_sent_max_words=20, l_diff=0.8, debug=False, vectors_ru1=None, vectors_ru2=None):\n",
    "    \n",
    "    splitted_ru = os.path.join(base_path, \"splitted\", os.path.basename(file_ru))\n",
    "    splitted_zh = os.path.join(base_path, \"splitted\", os.path.basename(file_zh))\n",
    "    proxy_ru = os.path.join(base_path, \"proxy\", os.path.basename(file_ru))    \n",
    "    ngramed_splitted_zh = os.path.join(base_path, \"gramed\", os.path.basename(file_zh))\n",
    "    ngramed_proxy_ru = os.path.join(base_path, \"gramed\", os.path.basename(file_ru))    \n",
    "    output_ru = os.path.join(base_path, \"done\", os.path.basename(file_ru))\n",
    "    output_zh = os.path.join(base_path, \"done\", os.path.basename(file_zh))\n",
    "    \n",
    "    print(\"Splitting ru.\")\n",
    "    with open(file_ru, mode='r', encoding='utf-8') as input_ru, open(splitted_ru, mode='w', encoding='utf-8') as out_ru:\n",
    "        ru = ' '.join(input_ru.readlines())\n",
    "        ru = re.sub(pattern_ru_orig, '', ru)\n",
    "        ru = re.sub(double_spaces, ' ', ru)\n",
    "        ru = re.sub(double_commas, ',', ru)\n",
    "        ru = re.sub(double_dash, '—', ru)\n",
    "\n",
    "        sent_ru = list(x.text for x in razdel.sentenize(ru))\n",
    "\n",
    "        count = 1\n",
    "        for x in sent_ru:\n",
    "            if count < len(sent_ru)-1:\n",
    "                out_ru.write(x.strip() + \"\\n\")\n",
    "            else:\n",
    "                out_ru.write(x.strip())\n",
    "            count += 1\n",
    "            \n",
    "    print(\"Russian text lines:\", len(sent_ru), \"\\n\")\n",
    "    \n",
    "    print(\"Splitting zh.\")\n",
    "    with open(file_zh, mode='r', encoding='utf-8') as input_zh, open(splitted_zh, mode='w', encoding='utf-8') as out_zh:\n",
    "        zh = ''.join(input_zh.readlines())    \n",
    "        zh = re.sub(pat_comma, '。', zh)\n",
    "\n",
    "        sent_zh = list(re.sub(pattern_zh,'',x.strip()) for x in split_zh(zh))\n",
    "\n",
    "        count = 1\n",
    "        for x in sent_zh:\n",
    "            if count < len(sent_zh)-1:\n",
    "                out_zh.write(x.strip() + \"\\n\")\n",
    "            else:\n",
    "                out_zh.write(x.strip())\n",
    "            count += 1\n",
    "\n",
    "    print(\"Chinese text lines:\", len(sent_zh), \"\\n\")    \n",
    "    \n",
    "    #check if proxy file exists\n",
    "    if not(os.path.isfile(proxy_ru)):\n",
    "        raise Exception(\"Proxy file not found:\", proxy_ru)\n",
    "    \n",
    "    print(\"Proxy file is found:\")\n",
    "    print(proxy_ru + \"\\n\")\n",
    "    \n",
    "    print(\"Creating ngrammed by sentence text files...\")\n",
    "    #ngramming\n",
    "#     max_sent_words = 20 #don't process very long sentences.\n",
    "    count_ngramed = 0\n",
    "    with open(splitted_zh, mode=\"r\", encoding=\"utf-8\") as input_zh, \\\n",
    "         open(proxy_ru, mode=\"r\", encoding=\"utf-8\") as input_proxy, \\\n",
    "         open(ngramed_splitted_zh, mode='w', encoding='utf-8') as ngram_zh, \\\n",
    "         open(ngramed_proxy_ru, mode='w', encoding='utf-8') as ngram_ru:\n",
    "        lines_zh = input_zh.readlines()\n",
    "        lines_ru_proxy = input_proxy.readlines()\n",
    "        file_len = len(lines_ru_proxy)\n",
    "        \n",
    "        for i in range(file_len):\n",
    "            #process sentence\n",
    "            #always write original sentence\n",
    "            ngram_ru.write(lines_ru_proxy[i].strip() + \"\\n\")\n",
    "            ngram_zh.write(lines_zh[i].strip() + \"\\n\")\n",
    "            count_ngramed += 1\n",
    "            #add ngamed sentences\n",
    "            for k in range(1, n_gram):\n",
    "                #write if every sentence is longer than max_sent_words\n",
    "                to_write = True\n",
    "                line_ru = \"\"\n",
    "                line_zh = \"\"\n",
    "                for j in range(k+1):\n",
    "                    if (len(re.sub(pattern_ru_letters_only,'',lines_ru_proxy[min(i+j, file_len-1)]).split())>n_gram_sent_max_words):\n",
    "                        to_write = False\n",
    "                    line_ru += lines_ru_proxy[min(i+j, file_len-1)].strip()\n",
    "                    line_ru += \" \"\n",
    "                    \n",
    "                    line_zh += lines_zh[min(i+j, file_len-1)].strip()\n",
    "                    \n",
    "                    #ngram_ru.write(lines_ru_proxy[min(i+j, file_len-1)].strip() + \" \")\n",
    "                    #ngram_zh.write(lines_zh[min(i+j, file_len-1)].strip() + \" \")\n",
    "                if to_write:\n",
    "                    ngram_ru.write(line_ru + \"\\n\")\n",
    "                    ngram_zh.write(line_zh + \"\\n\")\n",
    "                    count_ngramed += 1\n",
    "                    #ngram_zh.write(\"\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Ngrammed Chinese text lines:\", count_ngramed, \"\\n\")   \n",
    "#     raise Exception(\"debug\")\n",
    "    \n",
    "    #batching    \n",
    "    with open(splitted_ru, mode=\"r\", encoding=\"utf-8\") as input_ru, \\\n",
    "         open(ngramed_splitted_zh, mode=\"r\", encoding=\"utf-8\") as input_zh, \\\n",
    "         open(ngramed_proxy_ru, mode=\"r\", encoding=\"utf-8\") as input_proxy:\n",
    "        lines_ru = input_ru.readlines()\n",
    "        lines_zh = input_zh.readlines()\n",
    "        lines_ru_proxy = input_proxy.readlines()    \n",
    "    \n",
    "    batch_number = 0\n",
    "    total_pairs = 0\n",
    "    with open(output_ru, mode='w', encoding='utf-8') as out_ru, open(output_zh, mode='w', encoding='utf-8') as out_zh:\n",
    "        for lines_ru_batch, lines_ru_proxy_batch, lines_zh_batch in get_batch(lines_ru, lines_ru_proxy, lines_zh, batch_size):\n",
    "            batch_number += 1\n",
    "\n",
    "            #print(lines_ru_batch)\n",
    "            \n",
    "            print(\"--------------------------------------\")\n",
    "            print(\"batch:\", batch_number, \"[\", (batch_number-1) * batch_size, \"-\", batch_number * batch_size, \"]\", \"Calculating vectors...\")\n",
    "            vectors_ru1 = get_line_vectors(lines_ru_proxy_batch)\n",
    "            vectors_ru2 = get_line_vectors(lines_ru_batch)\n",
    "            \n",
    "#             if vectors_ru1 is None:\n",
    "#                 vectors_ru1 = get_line_vectors(lines_ru_proxy_batch)\n",
    "#             if vectors_ru2 is None:\n",
    "#                 vectors_ru2 = get_line_vectors(lines_ru_batch)\n",
    "\n",
    "            print(\"Calculating similarity matrix...\")\n",
    "            sim_matrix = get_sim_matrix(vectors_ru1, vectors_ru2, window)  \n",
    "\n",
    "            plt.figure(figsize=(12,10))\n",
    "            sns.heatmap(sim_matrix,cmap=\"Greens\", vmin=threshold)\n",
    "            plt.show()\n",
    "            \n",
    "            count = 0\n",
    "            for i in range(sim_matrix.shape[0]):\n",
    "                for j in range(sim_matrix.shape[1]):\n",
    "                    if sim_matrix[i,j] >= threshold:\n",
    "                        count += 1\n",
    "            total_pairs += count\n",
    "            print(\"\\nPairs found:\", count, \"\\n\")\n",
    "            print(\"\\nTotal pairs found:\", total_pairs, \"\\n\")\n",
    "            print(\"--------------------------------------\")\n",
    "            \n",
    "            res_ru, res_zh, res_ru_proxy, sims = get_pairs(lines_ru_batch, lines_zh_batch, lines_ru_proxy_batch, sim_matrix, threshold)\n",
    "\n",
    "            for x,y,z,s in zip(res_ru, res_zh, res_ru_proxy, sims):\n",
    "                #filter if res and proxy length is differ to much\n",
    "                x_len = len(re.sub(pattern_ru_letters_only,'',x).split())\n",
    "                z_len = len(re.sub(pattern_ru_letters_only,'',z).split())\n",
    "                xz_diff = min(x_len,z_len)/max(x_len,z_len)\n",
    "                if debug:\n",
    "                    print(\"----------------------------------------------------------------------------\")\n",
    "                    print(\"similarity:\", s)\n",
    "                    print(\"length diff:\", xz_diff)\n",
    "                    if xz_diff < l_diff:\n",
    "                        print(\"[filtered]\")\n",
    "                    print(\"--------------------------------------\")\n",
    "                \n",
    "                if (debug) | (xz_diff >= l_diff):\n",
    "                    out_ru.write(x)\n",
    "                    out_zh.write(y)                    \n",
    "                    print(x)\n",
    "                    print(z)\n",
    "                    print(y)\n",
    "\n",
    "    print(\"Done. Total pairs found:\", total_pairs)\n",
    "    \n",
    "    return vectors_ru1, vectors_ru2\n",
    "\n",
    "def get_batch(iter1, iter2, iter3, n):\n",
    "    l1 = len(iter1)\n",
    "    l3 = len(iter3)\n",
    "    k = int(round(n * l3/l1))    \n",
    "    kdx = 0 - k\n",
    "    for ndx in range(0, l1, n):\n",
    "        kdx += k\n",
    "        yield iter1[ndx:min(ndx + n, l1)], iter2[kdx:min(kdx + k, l3)], iter3[kdx:min(kdx + k, l3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting ru.\n",
      "Russian text lines: 241 \n",
      "\n",
      "Splitting zh.\n",
      "Chinese text lines: 311 \n",
      "\n",
      "Proxy file is found:\n",
      "D:\\ml\\tap\\proxy\\chekhov_futlar_ru.txt\n",
      "\n",
      "Creating ngrammed by sentence text files...\n",
      "Ngrammed Chinese text lines: 402 \n",
      "\n",
      "--------------------------------------\n",
      "batch: 1 [ 0 - 100 ] Calculating vectors...\n"
     ]
    }
   ],
   "source": [
    "base_path = r\"D:\\ml\\tap\"\n",
    "\n",
    "file_ru = r\"D:\\ml\\tap\\raw\\chekhov_futlar_ru.txt\"\n",
    "file_zh = r\"D:\\ml\\tap\\raw\\chekhov_futlar_zh.txt\"\n",
    "\n",
    "v1, v2 = align_texts(base_path, file_ru, file_zh, threshold=0.89, batch_size=100, window=12, n_gram=2, \\\n",
    "                     n_gram_sent_max_words=12, l_diff=0.6, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v1, v2 = align_texts(file_ru, file_zh, 0.85, v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
